{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking Dataset - Marketing Targets\n",
    "\n",
    "### Content\n",
    "\n",
    "- The data is related to the direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed by the customer or not. The data folder contains two datasets:-\n",
    "\n",
    "    - train.csv: 45,211 rows and 18 columns ordered by date (from May 2008 to November 2010)\n",
    "    - test.csv: 4521 rows and 18 columns with 10% of the examples (4521), randomly selected from train.csv\n",
    "\n",
    "#### Dataset Link : https://www.kaggle.com/datasets/rashmiranu/banking-dataset-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier \n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "\n",
    "df = pd.read_csv('data/train.csv', sep=';')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the information about the dataset\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are no missing values in the above dataframe\n",
    "- Lets check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "print(df.duplicated().sum())\n",
    "# len(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As there are no duplicate entries found there is not need to drop the duplicates\n",
    "    - df.drop_duplicates()\n",
    "\n",
    "- The above dataset looks mostly clean, so need for further cleaning of dataset\n",
    "- Lets try to visualize the dataframe using automated python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated plots for easy visuzlization\n",
    "\n",
    "AV = AutoViz_Class()\n",
    "profile_autoviz = AV.AutoViz('data/train.csv', sep=';', depVar='y', dfte='Data', header=0, verbose=1, lowess=False,\n",
    "               chart_format='html',save_plot_dir='AutoViz_Plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another report generating tool to analyze/visualize the reports in pandas\n",
    "# Pandas profiling\n",
    "profile_pandas = ProfileReport(df)\n",
    "profile_pandas.to_file('pandas_profile.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to convert the non-numerical columns to numerical columns for better analysis\n",
    "# There are multiple methods to convert the non-numerical columns to numerical columns (one-hot encoding, label-encoding)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check a sample of the dataframe after performing the encoding\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try to understand the distribution of the dataset using the describe function\n",
    "\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Columns like balance, housing, contact, duration is not normally distributed based on the mean, median and mode\n",
    "- So let us try to perform zscalar method for scaling the dataframe\n",
    "- Before scaling, let us split the dataframe to train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_features = df.drop('y', axis=1)\n",
    "target_feature = df[['y']]\n",
    "\n",
    "target_feature.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Target feature is highly imbalanced, so applying smote for balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = target_feature.value_counts()\n",
    "print('Before sampling: \\n', count)\n",
    "\n",
    "smote = SMOTE()\n",
    "independent_features_sampled, target_feature_sampled = smote.fit_resample(independent_features, target_feature)\n",
    "\n",
    "count_sam = target_feature_sampled.value_counts()\n",
    "print('After sampling: \\n', count_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_test_split (As the dataset is not very large, let us take the test_size to be 15% of the entire dataframe)\n",
    "# As we have sampled, startify is not necessary\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(independent_features_sampled, target_feature_sampled, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  shuffle=True, \n",
    "                                                  stratify=target_feature_sampled)\n",
    "\n",
    "print(X_train.shape, X_val.shape)\n",
    "print(y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us scale the features using z-scalar technique\n",
    "sc = StandardScaler() \n",
    "X_train_sc = sc.fit_transform(X_train) # Equivalent to X_train_ = (X_train - X_train.mean()) / X_train.std()\n",
    "X_val_sc = sc.fit_transform(X_val)\n",
    "\n",
    "# Checking the max and min value of the series\n",
    "print(X_train_sc.max(), X_train_sc.min())         # Gives the max and min values of all the features combined\n",
    "print(X_val_sc.max(), X_val_sc.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of scaled series\n",
    "X_train_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have scaled, we can apply the dataset on the algorithm\n",
    "- As it is a binary classification problem we will start with logistic regression and try all the classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='micro')\n",
    "    recall = recall_score(y_true, y_pred, average='micro')\n",
    "    return {'accuracy': round(acc, 2), 'precision': round(prec, 2), 'recall': round(recall, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "\n",
    "# Logistic Regression\n",
    "LG = LogisticRegression()\n",
    "LG.fit(X_train_sc, y_train)\n",
    "y_pred_lg = LG.predict(X_val_sc)\n",
    "\n",
    "# Gaussian NaiveBayes\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train_sc, y_train)\n",
    "y_pred_nb = NB.predict(X_val_sc)\n",
    "\n",
    "# # Support Vector Classifier\n",
    "SVM = SVC(C=0.8, kernel='linear', probability=True)\n",
    "SVM.fit(X_train_sc, y_train)\n",
    "y_pred_svm = SVM.predict(X_val_sc)\n",
    "\n",
    "# KNN\n",
    "n = list(np.arange(3,20,2))\n",
    "acc = []\n",
    "for k in n:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_sc, y_train)\n",
    "    # predict the response\n",
    "    y_pred = knn.predict(X_val_sc)\n",
    "    # evaluate accuracy\n",
    "    scores = accuracy_score(y_val, y_pred)\n",
    "    acc.append(scores)\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in acc]\n",
    "# determining the best 'k' value\n",
    "optimal_k = n[MSE.index(min(MSE))]\n",
    "# Training with optimal 'k'\n",
    "KNN = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "KNN.fit(X_train_sc, y_train)\n",
    "y_pred_knn = KNN.predict(X_val_sc)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train_sc, y_train)\n",
    "y_pred_dt = DT.predict(X_val_sc)\n",
    "\n",
    "# Bagging Classifier\n",
    "BG = BaggingClassifier()\n",
    "BG.fit(X_train_sc, y_train)\n",
    "y_pred_bg = BG.predict(X_val_sc)\n",
    "\n",
    "# Random Forest Classifier\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train_sc, y_train)\n",
    "y_pred_rf = RF.predict(X_val_sc)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "GB = GradientBoostingClassifier()\n",
    "GB.fit(X_train_sc, y_train)\n",
    "y_pred_gb = GB.predict(X_val_sc)\n",
    "\n",
    "# Ada Boost Classifier\n",
    "AB = AdaBoostClassifier()\n",
    "AB.fit(X_train_sc, y_train)\n",
    "y_pred_ab = AB.predict(X_val_sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metrics for all the defined models\n",
    "\n",
    "LG_model = get_metrics(y_val, y_pred_lg)\n",
    "NB_model = get_metrics(y_val, y_pred_nb)\n",
    "SVM_model = get_metrics(y_val, y_pred_svm)\n",
    "KNN_model = get_metrics(y_val, y_pred_knn)\n",
    "DT_model = get_metrics(y_val, y_pred_dt)\n",
    "BG_model = get_metrics(y_val, y_pred_bg)\n",
    "RF_model = get_metrics(y_val, y_pred_rf)\n",
    "GB_model = get_metrics(y_val, y_pred_gb)\n",
    "AB_model = get_metrics(y_val, y_pred_ab)\n",
    "\n",
    "model_metrics = {'Model_Name': ['LG_model', 'NB_model', 'SVM_model', 'KNN_model', 'DT_model', 'BG_model', 'RF_model', 'GB_model', 'AB_model'],\n",
    "                 'Model_Metrics': [LG_model, NB_model, SVM_model, KNN_model, DT_model, BG_model, RF_model, GB_model, AB_model]}\n",
    "\n",
    "model_df = pd.DataFrame(model_metrics)\n",
    "\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of all the methods Random Forest Classifier performs better on the pre-processed dataset\n",
    "# Let us apply some hyper parameter tuning on the Random Forest technique\n",
    "\n",
    "# Hyper parameter tuning\n",
    "\n",
    "def hyper_parameter_tuning(X_train, y_train):\n",
    "    # define random parameters grid\n",
    "    n_estimators = [5,21,51,101] # number of trees in the random forest\n",
    "    max_features = ['auto', 'sqrt'] # number of features in consideration at every split\n",
    "    max_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree\n",
    "    min_samples_split = [2, 6, 10] # minimum sample number to split a node\n",
    "    min_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node\n",
    "    bootstrap = [True, False] # method used to sample data points\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                    'max_features': max_features,\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split,\n",
    "                    'min_samples_leaf': min_samples_leaf,\n",
    "                    'bootstrap': bootstrap\n",
    "                  }\n",
    "    \n",
    "    classifier = RandomForestClassifier()\n",
    "    model_tuning = RandomizedSearchCV(estimator = classifier, param_distributions = random_grid,\n",
    "                   n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = 1)\n",
    "    model_tuning.fit(X_train, y_train)\n",
    "\n",
    "    print ('Random grid: ', random_grid, '\\n')\n",
    "    # print the best parameters\n",
    "    print ('Best Parameters: ', model_tuning.best_params_, ' \\n')\n",
    "\n",
    "    best_params = model_tuning.best_params_\n",
    "    \n",
    "    n_estimators = best_params['n_estimators']\n",
    "    min_samples_split = best_params['min_samples_split']\n",
    "    min_samples_leaf = best_params['min_samples_leaf']\n",
    "    max_features = best_params['max_features']\n",
    "    max_depth = best_params['max_depth']\n",
    "    bootstrap = best_params['bootstrap']\n",
    "    \n",
    "    model_tuned = RandomForestClassifier(n_estimators = n_estimators, min_samples_split = min_samples_split,\n",
    "                                         min_samples_leaf= min_samples_leaf, max_features = max_features,\n",
    "                                         max_depth= max_depth, bootstrap=bootstrap) \n",
    "    model_tuned.fit( X_train, y_train)\n",
    "\n",
    "    return model_tuned,best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_model, best_params = hyper_parameter_tuning(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the output with the tuned model\n",
    "\n",
    "y_pred = Tuned_model.predict(X_val_sc)\n",
    "get_metrics(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "with open('model/tuned_model.pkl', 'wb') as file:\n",
    "    pickle.dump(Tuned_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset and then pre=process it before making the prediction\n",
    "\n",
    "df_test = pd.read_csv('data/test.csv', sep=';')\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "for col in df_test.columns:\n",
    "    if df_test[col].dtype == 'object':\n",
    "        df_test[col] = le.transform(df_test[col])\n",
    "\n",
    "# Convering the dataframe to series\n",
    "X_test = df_test.drop('y', axis=1)\n",
    "y_test = df_test[['y']]\n",
    "\n",
    "# Scaling\n",
    "# Let us scale the features using z-scalar technique\n",
    "sc = StandardScaler() \n",
    "X_test_sc = sc.fit_transform(X_test) # Equivalent to X_train_ = (X_train - X_train.mean()) / X_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved model\n",
    "with open('model/tuned_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "y_pred = loaded_model.predict(X_test_sc)\n",
    "get_metrics(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
